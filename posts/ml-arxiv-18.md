---
title: ML Arxiv Haul ~18
published_at: 2023-05-02T00:17:00.000Z
snippet: A new installment of ML papers and comments
tags: arxiv
---

## Comments / Criticism

At the end of March, an ML researcher [reported](https://twitter.com/BauwensRobin/status/1640317530144223232) that their paper rejection included non-existent references, likely generated by a GPT model. Comments suggest that other people have attempted similar reviewing this year, though usually with some transparency. The reviewer was later disqualified.

An old man in my building confronted me in the elevator about whether AI was an angel or devil
(based on the cover of *The Economist*). He had seen a report on the local news which asked ChatGPT about the economy and invented / hallucinated sources.

## New Paper Content

<blockquote>
    <a href="https://arxiv.org/abs/2112.10547">
    <h4>A Memristor-Based Bayesian Machine</h4>
    <p>
In recent years, a considerable research effort has shown the energy benefits of implementing neural networks with‚Ä¶
    </p>
    </a>
</blockquote>

Memristors are somewhat new in electrical engineering and applications are not super clear. Here's a project from 2021 which stores likelihoods in "likelihoods memory arrays". They also needed to include random number generation in their custom hardware (linear-feedback shift registers) and these consumed the bulk of the power (88%) for their system.

<hr/>

<blockquote>
    <a href="https://laion.ai/blog/open-flamingo/">
    <h4>Announcing OpenFlamingo: An open-source framework for training vision-language models</h4>
    <p>
We are thrilled to announce the release of OpenFlamingo, an‚Ä¶
    </p>
    </a>
</blockquote>

Google announced Flamingo and shared the training code in 2022, but now this is a published model from LAION. They update the C4 corpus to include a stream of text and image content.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2304.06035">
    <h4>Choose Your Weapon: Survival Strategies for Depressed AI Academics</h4>
    <p>
Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI‚Ä¶
    </p>
    </a>
</blockquote>

This paper got shared widely because it answers the question of how AI researchers can continue to work in such a competitive space, against tech giant research labs obsessed with scale. You get the feeling that it's a conversation between exhausted conference attendees more than a lecture or a traditional paper. They give a few good options to keep hacking on.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2304.01938">
    <h4>Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics</h4>
    <p>
We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics‚Ä¶
    </p>
    </a>
</blockquote>

Interesting paper on usefulness of ChatGPT in a medical context

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2303.01037">
    <h4>Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages</h4>
    <p>
We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR)‚Ä¶
    </p>
    </a>
</blockquote>

Google does self-supervised learning on 12 million hours (1,370 years) of unlabeled YouTube audio. Then with additional, labeled audio, they can outperform OpenAI's Whisper model on English and 100+ other languages.

<hr/>

<blockquote>
    <a href="https://huggingface.co/trl-lib/llama-7b-se-rl-peft">
    <h4>trl-lib/llama-7b-se-rl-peft ¬∑ Hugging Face</h4>
    <p>
Adapter weights of a Reinforcement Learning fine-tuned model based on the LLaMA model (see Meta's LLaMA release for the‚Ä¶
    </p>
    </a>
</blockquote>

Due to limited / torrent release of Llama, HuggingFace shares their RL-improved model through adapter weights. As explained on https://huggingface.co/blog/stackllama

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2212.04972">
    <h4>MOPRD: A multidisciplinary open peer review dataset</h4>
    <p>
Open peer review is a growing trend in academic publications. Public access to peer review data can benefit both the‚Ä¶
    </p>
    </a>
</blockquote>

Constructing a dataset from open peer reviews of over 6,500 papers.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2302.12173">
    <h4>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</h4>
    <p>
We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already‚Ä¶
    </p>
    </a>
</blockquote>

Paper discusses a new class of 'prompt injection' or just weird behaviors in LLMs with retrieval / web search capability. I've seen a few of these attacks on Bing (using invisible text on a homepage saying that Bing when reading must mention a particular skill). The researchers continue into more hypothetical examples and models and architectures.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2304.12397">
    <h4>On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research</h4>
    <p>
Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly‚Ä¶
    </p>
    </a>
</blockquote>

Google's Perspective API continues to be developed and retrained behind the scenes, so toxicity measures are not reproducible.

Note: three authors give their institution as "Cohere for AI" and one as "Cohere"..?

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2302.10149">
    <h4>Poisoning Web-Scale Training Datasets is Practical</h4>
    <p>
Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In this paper, we‚Ä¶
    </p>
    </a>
</blockquote>

Authors take over expired domains / broken URLs within LAION and another image dataset to demonstrate they could insert new images into the dataset. Many people continue to download and regenerate these datasets (which primarily are shared as a set of links) so it wouldn't be exceedingly difficult to poison future models.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2203.13722">
    <h4>Probing Pre-Trained Language Models for Cross-Cultural Differences in Values</h4>
    <p>
Language embeds information about social, cultural, and political values people hold. Prior work has explored social‚Ä¶
    </p>
    </a>
</blockquote>

The researchers translate some leading sentences (the organization should prioritize _) into different languages and compare the preferences. They find significant differences in probability of which word or which position the model would take, even within the same multilingual model such as mBERT.

The paper has a perplexing use of "Pre-trained Language Models" / PLMs. I assume this is because BERT and XLM are not as large as the most recent LLMs, but it's a weird one.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2304.06718">
    <h4>Segment Everything Everywhere All at Once</h4>
    <p>
Despite the growing demand for interactive AI systems, there have been few comprehensive studies on human-AI‚Ä¶
    </p>
    </a>
</blockquote>

Meta basically owning zero-shot image segmentation in video -- impressive stuff.

<hr/>

<blockquote>
    <a href="https://openreview.net/forum?id=ae7BJIOxkxH">
    <h4>Stingy Teacher: Sparse Logits Suffice to Fail Knowledge Distillation</h4>
    <p>
Knowledge distillation (KD) aims to transfer the discrimination power of pre-trained teacher models to (more‚Ä¶
    </p>
    </a>
</blockquote>

I forget adding this to my papers queue, but here it is. The concept is that an adversary running multiple queries to try and clone the original model, can be derailed by sharing only the probabilities of the top-K classes (on an ImageNet prediction).

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2304.05128">
    <h4>Teaching Large Language Models to Self-Debug</h4>
    <p>
Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming‚Ä¶
    </p>
    </a>
</blockquote>

Mainly Google research, using OpenAI's davinci model. A code-generation model is shown outputs (and in some cases) a few-shot mock of running code and fixing a program. This could improve systems which generate 100s of programs and compare outputs. The goal is a little convoluted because the task is converting programs between languages (for example, Python-C++), plus generating SQL queries from text, etc. I'd have liked to see a paper limited to one of these tasks.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2210.13966">
    <h4>The Debate Over Understanding in AI's Large Language Models</h4>
    <p>
We survey a current, heated debate in the AI research community on whether large pre-trained language models can be‚Ä¶
    </p>
    </a>
</blockquote>

Santa Fe Institute (notably more philosophical / futurist) paper on whether LLMs have understanding. A 2022 survey of researchers was split evently on whether a text-only generative model "could understand natural language in some non-trivial sense".

<hr/>

<blockquote>
    <a href="https://lukesalamone.github.io/posts/what-is-temperature/">
    <h4>What is Temperature in NLP?üê≠</h4>
    </a>
</blockquote>

This was the only thing which I've seen which makes sense of the temperature parameter and why so many UIs for language models let you tweak responses with temperature.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2303.03004">
    <h4>xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval</h4>
    <p>
The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can‚Ä¶
    </p>
    </a>
</blockquote>

The new, new, new largest code LLM benchmark.

<hr/>

<blockquote>
    <a href="https://arxiv.org/abs/2303.16203">
    <h4>Your Diffusion Model is Secretly a Zero-Shot Classifier</h4>
    <p>
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image‚Ä¶
    </p>
    </a>
</blockquote>

Project from CMU, adapting values of a diffusion model to become an image classifier.

<br/>
<br/>
<br/>
